{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# sklearn Machine Learning Examples\n",
        "\n",
        "This notebook demonstrates classification, regression, dimensionality reduction, and clustering using sklearn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Classification Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install scikit-learn numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name '_spbase' from 'scipy.sparse._base' (/Users/elsa/miniconda3/envs/py311env/lib/python3.11/site-packages/scipy/sparse/_base.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VarianceThreshold  \u001b[38;5;66;03m# Correct import\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311env/lib/python3.11/site-packages/sklearn/__init__.py:70\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __check_build, _distributor_init  \u001b[38;5;66;03m# noqa: E402 F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     73\u001b[39m _submodules = [\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    112\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311env/lib/python3.11/site-packages/sklearn/base.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_pandas_na, is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_repr_html\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReprHTMLMixin, _HTMLDocumentationLinkMixin\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311env/lib/python3.11/site-packages/sklearn/utils/__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing, resample, shuffle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311env/lib/python3.11/site-packages/sklearn/utils/_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311env/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311env/lib/python3.11/site-packages/scipy/sparse/__init__.py:310\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dok\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_coo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py311env/lib/python3.11/site-packages/scipy/sparse/_lil.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray, issparse\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_index\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexMixin, INT_TYPES, _broadcast_arrays\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (getdtype, isshape, isscalarlike, upcast_scalar,\n\u001b[32m     16\u001b[39m                        check_shape, check_reshape_kwargs)\n",
            "\u001b[31mImportError\u001b[39m: cannot import name '_spbase' from 'scipy.sparse._base' (/Users/elsa/miniconda3/envs/py311env/lib/python3.11/site-packages/scipy/sparse/_base.py)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import VarianceThreshold  # Correct import\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "selector = VarianceThreshold(threshold=0.0)\n",
        "X_temp = df.drop('Survived', axis=1)\n",
        "selector.fit(X_temp)\n",
        "selected_features = X_temp.columns[selector.get_support()]\n",
        "df = df[list(selected_features) + ['Survived']]\n",
        "\n",
        "correlation_matrix = df.drop('Survived', axis=1).corr().abs()\n",
        "upper_triangle = correlation_matrix.where(\n",
        "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
        "df = df.drop(columns=high_corr_features)\n",
        "\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_pred = lr_model.predict(X_test_scaled)\n",
        "lr_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = {\n",
        "    'Model': ['Logistic Regression', 'Random Forest'],\n",
        "    'Accuracy': [\n",
        "        accuracy_score(y_test, lr_pred),\n",
        "        accuracy_score(y_test, rf_pred)\n",
        "    ],\n",
        "    'Precision': [\n",
        "        precision_score(y_test, lr_pred),\n",
        "        precision_score(y_test, rf_pred)\n",
        "    ],\n",
        "    'Recall': [\n",
        "        recall_score(y_test, lr_pred),\n",
        "        recall_score(y_test, rf_pred)\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        f1_score(y_test, lr_pred),\n",
        "        f1_score(y_test, rf_pred)\n",
        "    ],\n",
        "    'AUC-ROC': [\n",
        "        roc_auc_score(y_test, lr_pred_proba),\n",
        "        roc_auc_score(y_test, rf_pred_proba)\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(metrics)\n",
        "print(comparison_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Regression Example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Data Loading and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.feature_selection import VarianceThreshold  \n",
        "\n",
        "house_df = pd.read_csv('https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv')\n",
        "house_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_df = pd.get_dummies(house_df, columns=['ocean_proximity'], drop_first=True)\n",
        "\n",
        "house_df = house_df.select_dtypes(include=[np.number])\n",
        "\n",
        "selector_house = VarianceThreshold(threshold=0.0)\n",
        "X_temp_house = house_df.drop('median_house_value', axis=1)\n",
        "selector_house.fit(X_temp_house)\n",
        "selected_features_house = X_temp_house.columns[selector_house.get_support()]\n",
        "house_df = house_df[list(selected_features_house) + ['median_house_value']]\n",
        "\n",
        "correlation_matrix = house_df.drop('median_house_value', axis=1).corr().abs()\n",
        "upper_triangle = correlation_matrix.where(\n",
        "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
        "house_df = house_df.drop(columns=high_corr_features)\n",
        "\n",
        "X_house = house_df.drop('median_house_value', axis=1)\n",
        "y_house = house_df['median_house_value']\n",
        "X_train_house, X_test_house, y_train_house, y_test_house = train_test_split(\n",
        "    X_house, y_house, test_size=0.2, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 KNN Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "knn_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('power_transformer', PowerTransformer(method='yeo-johnson')),\n",
        "    ('knn', KNeighborsRegressor(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn_pipeline.fit(X_train_house, y_train_house)\n",
        "knn_pred = knn_pipeline.predict(X_test_house)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Linear Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_lr = X_train_house.fillna(X_train_house.median())\n",
        "X_test_lr = X_test_house.fillna(X_train_house.median())\n",
        "\n",
        "scaler_lr = StandardScaler()\n",
        "X_train_lr_scaled = scaler_lr.fit_transform(X_train_lr)\n",
        "X_test_lr_scaled = scaler_lr.transform(X_test_lr)\n",
        "\n",
        "correlation_matrix_lr = pd.DataFrame(X_train_lr_scaled).corr().abs()\n",
        "upper_triangle_lr = correlation_matrix_lr.where(\n",
        "    np.triu(np.ones(correlation_matrix_lr.shape), k=1).astype(bool)\n",
        ")\n",
        "high_corr_features_lr = [column for column in upper_triangle_lr.columns if any(upper_triangle_lr[column] > 0.95)]\n",
        "X_train_lr_scaled = pd.DataFrame(X_train_lr_scaled).drop(columns=high_corr_features_lr).values\n",
        "X_test_lr_scaled = pd.DataFrame(X_test_lr_scaled).drop(columns=high_corr_features_lr).values\n",
        "\n",
        "lr_reg_model = LinearRegression()\n",
        "lr_reg_model.fit(X_train_lr_scaled, y_train_house)\n",
        "lr_reg_pred = lr_reg_model.predict(X_test_lr_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "regression_metrics = {\n",
        "    'Model': ['KNN Regression', 'Linear Regression'],\n",
        "    'RMSE': [\n",
        "        np.sqrt(mean_squared_error(y_test_house, knn_pred)),\n",
        "        np.sqrt(mean_squared_error(y_test_house, lr_reg_pred))\n",
        "    ],\n",
        "    'MAE': [\n",
        "        mean_absolute_error(y_test_house, knn_pred),\n",
        "        mean_absolute_error(y_test_house, lr_reg_pred)\n",
        "    ],\n",
        "    'R2': [\n",
        "        r2_score(y_test_house, knn_pred),\n",
        "        r2_score(y_test_house, lr_reg_pred)\n",
        "    ],\n",
        "    'MAPE': [\n",
        "        mean_absolute_percentage_error(y_test_house, knn_pred),\n",
        "        mean_absolute_percentage_error(y_test_house, lr_reg_pred)\n",
        "    ]\n",
        "}\n",
        "\n",
        "regression_comparison_df = pd.DataFrame(regression_metrics)\n",
        "print(regression_comparison_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion: are these a good models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PCA with Normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca_df = house_df.copy()\n",
        "X_pca = pca_df.drop('median_house_value', axis=1)\n",
        "X_pca = X_pca.fillna(X_pca.median())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Normalization and PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler_pca = StandardScaler()\n",
        "X_pca_scaled = scaler_pca.fit_transform(X_pca)\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca_transformed = pca.fit_transform(X_pca_scaled)\n",
        "\n",
        "print(f\"Original features: {X_pca_scaled.shape[1]}\")\n",
        "print(f\"PCA components (95% variance): {X_pca_transformed.shape[1]}\")\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Variance Explained Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca_2d = PCA(n_components=2)\n",
        "X_pca_2d = pca_2d.fit_transform(X_pca_scaled)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.6, s=30)\n",
        "axes[0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
        "axes[0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
        "axes[0].set_title('PCA - First Two Principal Components')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[0].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "\n",
        "n_components_to_show = min(10, len(pca.explained_variance_ratio_))\n",
        "axes[1].bar(range(1, n_components_to_show + 1), \n",
        "            pca.explained_variance_ratio_[:n_components_to_show])\n",
        "axes[1].set_xlabel('Principal Component')\n",
        "axes[1].set_ylabel('Explained Variance Ratio')\n",
        "axes[1].set_title('Explained Variance by Component')\n",
        "axes[1].set_xticks(range(1, n_components_to_show + 1))\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "cumsum_var = np.cumsum(pca.explained_variance_ratio_[:n_components_to_show])\n",
        "ax2 = axes[1].twinx()\n",
        "ax2.plot(range(1, n_components_to_show + 1), cumsum_var, 'r-', marker='o', label='Cumulative')\n",
        "ax2.set_ylabel('Cumulative Explained Variance', color='r')\n",
        "ax2.tick_params(axis='y', labelcolor='r')\n",
        "ax2.axhline(y=0.95, color='r', linestyle='--', alpha=0.5, label='95% threshold')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Variance explained by PC1: {pca_2d.explained_variance_ratio_[0]:.4f}\")\n",
        "print(f\"Variance explained by PC2: {pca_2d.explained_variance_ratio_[1]:.4f}\")\n",
        "print(f\"Total variance (first 2 components): {pca_2d.explained_variance_ratio_.sum():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. K-Means Clustering with Elbow Method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.feature_selection import VarianceThreshold  \n",
        "\n",
        "kmeans_df = house_df.copy()\n",
        "X_kmeans = kmeans_df.drop('median_house_value', axis=1)\n",
        "X_kmeans = X_kmeans.fillna(X_kmeans.median())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_kmeans = X_kmeans.select_dtypes(include=[np.number])\n",
        "\n",
        "selector_km = VarianceThreshold(threshold=0.0)\n",
        "selector_km.fit(X_kmeans)\n",
        "selected_features_km = X_kmeans.columns[selector_km.get_support()]\n",
        "X_kmeans = X_kmeans[selected_features_km]\n",
        "\n",
        "correlation_matrix_km = X_kmeans.corr().abs()\n",
        "upper_triangle_km = correlation_matrix_km.where(\n",
        "    np.triu(np.ones(correlation_matrix_km.shape), k=1).astype(bool)\n",
        ")\n",
        "high_corr_features_km = [column for column in upper_triangle_km.columns if any(upper_triangle_km[column] > 0.95)]\n",
        "X_kmeans = X_kmeans.drop(columns=high_corr_features_km)\n",
        "\n",
        "scaler_km = StandardScaler()\n",
        "transformer_km = PowerTransformer(method='yeo-johnson')\n",
        "\n",
        "X_kmeans_scaled = scaler_km.fit_transform(X_kmeans)\n",
        "X_kmeans_transformed = transformer_km.fit_transform(X_kmeans_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Elbow Method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inertias = []\n",
        "K_range = range(1, 20)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_kmeans_transformed)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(K_range, inertias, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Final K-Means Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimal_k = 2\n",
        "final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "clusters = final_kmeans.fit_predict(X_kmeans_transformed)\n",
        "\n",
        "silhouette_avg = silhouette_score(X_kmeans_transformed, clusters)\n",
        "print(f\"Optimal k: {optimal_k}\")\n",
        "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
        "print(f\"Cluster sizes: {np.bincount(clusters)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_kmeans['Cluster'] = clusters\n",
        "\n",
        "cluster_summary = X_kmeans.groupby('Cluster').agg(['mean', 'std', 'count'])\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CLUSTER CHARACTERISTICS - Mean Values\")\n",
        "print(\"=\" * 80)\n",
        "for cluster_id in range(optimal_k):\n",
        "    print(f\"\\n--- Cluster {cluster_id} (n={np.sum(clusters == cluster_id)}) ---\")\n",
        "    cluster_means = X_kmeans[X_kmeans['Cluster'] == cluster_id].drop('Cluster', axis=1).mean()\n",
        "    print(cluster_means.sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cluster_profiles = X_kmeans.groupby('Cluster').mean()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "cluster_profiles_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(cluster_profiles.T).T,\n",
        "    index=cluster_profiles.index,\n",
        "    columns=cluster_profiles.columns\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "sns.heatmap(cluster_profiles.T, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, ax=axes[0], cbar_kws={'label': 'Mean Value'})\n",
        "axes[0].set_title('Cluster Profiles - Raw Mean Values')\n",
        "axes[0].set_xlabel('Cluster')\n",
        "axes[0].set_ylabel('Features')\n",
        "\n",
        "# Heatmap 2: Standardized values (z-scores)\n",
        "sns.heatmap(cluster_profiles_scaled.T, annot=True, fmt='.2f', cmap='RdBu_r', \n",
        "            center=0, ax=axes[1], cbar_kws={'label': 'Z-Score'})\n",
        "axes[1].set_title('Cluster Profiles - Standardized Values (Z-scores)')\n",
        "axes[1].set_xlabel('Cluster')\n",
        "axes[1].set_ylabel('Features')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FEATURE IMPORTANCE FOR CLUSTERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "feature_importance = cluster_profiles.var(axis=0).sort_values(ascending=False)\n",
        "print(\"\\nFeatures with highest variance across clusters (most discriminative):\")\n",
        "print(feature_importance)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DETAILED CLUSTER COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "comparison_df = pd.DataFrame()\n",
        "for i in range(optimal_k):\n",
        "    comparison_df[f'Cluster_{i}'] = X_kmeans[X_kmeans['Cluster'] == i].drop('Cluster', axis=1).mean()\n",
        "\n",
        "comparison_df = comparison_df.T\n",
        "print(comparison_df.round(2))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DISTINCTIVE FEATURES PER CLUSTER\")\n",
        "print(\"=\" * 80)\n",
        "for cluster_id in range(optimal_k):\n",
        "    print(f\"\\n--- Cluster {cluster_id} ---\")\n",
        "    cluster_data = cluster_profiles_scaled.loc[cluster_id].sort_values(ascending=False)\n",
        "    high_features = cluster_data[cluster_data > 1.5].index.tolist()\n",
        "    low_features = cluster_data[cluster_data < -1.5].index.tolist()\n",
        "    \n",
        "    if high_features:\n",
        "        print(f\"Notably HIGH: {', '.join(high_features)}\")\n",
        "    if low_features:\n",
        "        print(f\"Notably LOW: {', '.join(low_features)}\")\n",
        "    if not high_features and not low_features:\n",
        "        print(\"No extremely distinctive features (moderate values)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca_viz = PCA(n_components=2)\n",
        "X_pca_viz = pca_viz.fit_transform(X_kmeans_transformed)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X_pca_viz[:, 0], X_pca_viz[:, 1], \n",
        "                     c=clusters, cmap='viridis', alpha=0.6, s=50)\n",
        "plt.xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.2%} variance)')\n",
        "plt.ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.2%} variance)')\n",
        "plt.title('K-Means Clusters in PCA Space')\n",
        "plt.colorbar(scatter, label='Cluster', ticks=range(optimal_k))\n",
        "\n",
        "# Add cluster centers\n",
        "centers_pca = pca_viz.transform(final_kmeans.cluster_centers_)\n",
        "plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
        "           c='red', marker='X', s=200, edgecolors='black', linewidth=2,\n",
        "           label='Centroids')\n",
        "\n",
        "# Add cluster labels\n",
        "for i, (x, y) in enumerate(centers_pca):\n",
        "    plt.annotate(f'C{i}', (x, y), fontsize=12, fontweight='bold',\n",
        "                ha='center', va='center', color='white')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Clean up - remove the cluster column if you want to keep original data intact\n",
        "X_kmeans = X_kmeans.drop('Cluster', axis=1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py311env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
