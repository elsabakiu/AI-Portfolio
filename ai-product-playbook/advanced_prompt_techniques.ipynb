{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Prompt Engineering Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Prompt Engineering Techniques\n",
        "\n",
        "Welcome to the hands-on exploration of advanced prompt engineering! In this notebook, you'll see these techniques in action and understand why prompt engineering is an **empirical science** that requires experimentation and iteration.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- Core prompting techniques (zero-shot, few-shot, one-shot, role, emotional, chain-of-thought)\n",
        "- Why LLM outputs are non-deterministic and how to control consistency\n",
        "- Automatic prompt generation and meta-prompting strategies\n",
        "- Token optimization techniques to reduce costs\n",
        "- Model biases and how they affect outputs\n",
        "- Using LLMs as judges to evaluate responses\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting, make sure you have:\n",
        "- OpenAI API key set up\n",
        "- Basic understanding of prompt engineering fundamentals (see lesson m2_01)\n",
        "- Python environment with openai library installed\n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Configuration\n",
        "\n",
        "First, let's import the necessary libraries and configure our OpenAI client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from collections import Counter\n",
        "import time\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Set default model\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "print(\"‚úÖ Setup complete! OpenAI client initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Functions\n",
        "\n",
        "Let's create some utility functions to make our experiments easier to manage and visualize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_openai(prompt, system_message=\"You are a helpful assistant.\", temperature=0.7, max_tokens=None, seed=None):\n",
        "    \"\"\"Helper function to call OpenAI API with specified parameters\"\"\"\n",
        "    try:\n",
        "        params = {\n",
        "            \"model\": MODEL,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            \"temperature\": temperature\n",
        "        }\n",
        "        \n",
        "        if max_tokens:\n",
        "            params[\"max_tokens\"] = max_tokens\n",
        "        if seed is not None:\n",
        "            params[\"seed\"] = seed\n",
        "            \n",
        "        response = client.chat.completions.create(**params)\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def display_response(title, response, params=None):\n",
        "    \"\"\"Display a response with formatting\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìù {title}\")\n",
        "    if params:\n",
        "        print(f\"Parameters: {params}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(response)\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "def count_tokens_approx(text):\n",
        "    \"\"\"Approximate token count (roughly 4 characters per token)\"\"\"\n",
        "    return len(text) // 4\n",
        "\n",
        "print(\"‚úÖ Helper functions loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Core Prompting Techniques\n",
        "\n",
        "Let's explore the fundamental prompting techniques you learned about in the lesson. Each example demonstrates when and how to use these techniques effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-Shot Prompting\n",
        "\n",
        "Zero-shot prompting provides a clear, detailed description of what you need **without examples**. This is the most commonly used prompting technique in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-shot: Just a clear instruction\n",
        "prompt = \"Write a haiku about AI Consulting and Integration\"\n",
        "\n",
        "response = call_openai(prompt, temperature=0.7)\n",
        "display_response(\"Zero-Shot Prompting\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One-Shot Prompting\n",
        "\n",
        "One-shot prompting provides **a single example** to demonstrate the expected output format. This helps the model understand the pattern you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Q: What is prompt engineering?\n",
        "A: Prompt engineering is the practice of crafting effective input prompts to guide AI models toward desired outputs or behaviors.\n",
        "\n",
        "Q: Why is prompt engineering useful?\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "response = call_openai(prompt, temperature=0.7)\n",
        "display_response(\"One-Shot Prompting\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-Shot Prompting\n",
        "\n",
        "Few-shot prompting adds **multiple examples** of the desired output format to guide the model's response, helping it understand the pattern more clearly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Few-shot: Provide multiple examples\n",
        "prompt = \"\"\"Q: What is prompt engineering?\n",
        "A: Prompt engineering is the practice of crafting effective input prompts to guide AI models toward desired outputs or behaviors.\n",
        "\n",
        "Q: Why is prompt engineering useful?\n",
        "A: Prompt engineering optimizes AI model outputs, enhancing accuracy, relevance, and task-specific performance.\n",
        "\n",
        "Q: What is temperature in AI models?\n",
        "A: Temperature controls the randomness of AI outputs, with lower values producing consistent results and higher values generating creative variations.\n",
        "\n",
        "Q: What is chain-of-thought prompting?\n",
        "A:\"\"\"\n",
        "\n",
        "response = call_openai(prompt, temperature=0.7)\n",
        "display_response(\"Few-Shot Prompting\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Role Prompting\n",
        "\n",
        "Role prompting asks the model to play a specific role or persona. Research shows it's more effective to define the assistant in **third person** (\"they are a senior data scientist\") rather than first person (\"you are a senior data scientist\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Role prompting: Third person (more effective)\n",
        "system_message = \"The assistant is a senior data scientist with 15 years of experience in machine learning and statistical analysis.\"\n",
        "prompt = \"Explain the bias-variance tradeoff in simple terms.\"\n",
        "\n",
        "response = call_openai(prompt, system_message=system_message, temperature=0.7)\n",
        "display_response(\"Role Prompting (Third Person)\", response, {\"role\": \"Senior Data Scientist\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Emotional Prompting\n",
        "\n",
        "Emotional prompting uses emotional stimuli to help the LLM better handle emotion and tone in its responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Emotional prompting: Adding emotional context\n",
        "prompt = \"\"\"This is very important to my career. I need you to explain AI ethics in a way that will help me \n",
        "succeed in my upcoming presentation. Please be thorough and thoughtful.\"\"\"\n",
        "\n",
        "response = call_openai(prompt, temperature=0.7)\n",
        "display_response(\"Emotional Prompting\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chain-of-Thought Prompting\n",
        "\n",
        "Chain-of-thought provides **step-by-step reasoning instructions**, enabling models to tackle complex tasks through structured thinking. This technique gave origin to reasoning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chain-of-thought: Request step-by-step reasoning\n",
        "prompt = \"\"\"A store has 23 apples. They receive a shipment of 47 more apples, but 15 are bruised and must be discarded. \n",
        "They sell 38 apples. How many apples do they have left? \n",
        "\n",
        "Please solve this step-by-step, showing your reasoning at each stage.\"\"\"\n",
        "\n",
        "response = call_openai(prompt, temperature=0)\n",
        "display_response(\"Chain-of-Thought Prompting\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Non-Deterministic Behavior Study\n",
        "\n",
        "One of the most important concepts in prompt engineering is understanding that **LLM outputs are non-deterministic**. This means that running the same prompt multiple times can produce different results, even with identical parameters.\n",
        "\n",
        "Let's explore this empirically!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part A: Demonstrating Inconsistency\n",
        "\n",
        "We'll run the exact same prompt with identical parameters **10 times** to show how the outputs vary. This demonstrates why prompt engineering requires testing and iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the same prompt 10 times with temperature=0.7\n",
        "prompt = \"Write a haiku about AI\"\n",
        "temperature = 0.7\n",
        "num_runs = 10\n",
        "\n",
        "print(f\"Running the same prompt {num_runs} times with temperature={temperature}\")\n",
        "print(f\"Prompt: '{prompt}'\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "responses = []\n",
        "for i in range(num_runs):\n",
        "    response = call_openai(prompt, temperature=temperature)\n",
        "    responses.append(response)\n",
        "    print(f\"\\nüîÑ Run #{i+1}:\")\n",
        "    print(response)\n",
        "    print(\"-\"*80)\n",
        "    time.sleep(0.5)  # Small delay to avoid rate limiting\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä Analysis:\")\n",
        "print(f\"Total unique responses: {len(set(responses))} out of {num_runs}\")\n",
        "print(f\"Average length: {sum(len(r) for r in responses) / len(responses):.0f} characters\")\n",
        "print(\"\\nüí° Key Insight: Even with the same prompt and parameters, we get different outputs!\")\n",
        "print(\"This is why prompt engineering is an empirical science - you must test and iterate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part B: Achieving Consistency\n",
        "\n",
        "Now let's explore two ways to get **consistent and reproducible** results:\n",
        "1. **Temperature = 0**: Produces the most deterministic outputs\n",
        "2. **Seed parameter**: Ensures reproducible results across runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: Temperature = 0 for consistency\n",
        "print(\"Method 1: Using temperature=0 for maximum consistency\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "prompt = \"Write a haiku about AI\"\n",
        "responses_temp0 = []\n",
        "\n",
        "for i in range(5):\n",
        "    response = call_openai(prompt, temperature=0)\n",
        "    responses_temp0.append(response)\n",
        "    print(f\"\\nRun #{i+1}: {response}\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"üìä Unique responses with temperature=0: {len(set(responses_temp0))} out of 5\")\n",
        "print(\"üí° Temperature=0 produces highly consistent (though not always identical) results!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: Using seed for reproducibility\n",
        "print(\"\\nMethod 2: Using seed parameter for reproducibility\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "prompt = \"Write a haiku about AI\"\n",
        "seed_value = 42\n",
        "responses_seeded = []\n",
        "\n",
        "for i in range(5):\n",
        "    response = call_openai(prompt, temperature=0.7, seed=seed_value)\n",
        "    responses_seeded.append(response)\n",
        "    print(f\"\\nRun #{i+1}: {response}\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"üìä Unique responses with seed={seed_value}: {len(set(responses_seeded))} out of 5\")\n",
        "print(\"üí° Using a seed parameter gives you reproducible results!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### When to Use Consistency vs. Variation\n",
        "\n",
        "**Use temperature=0 or seed when:**\n",
        "- Testing and debugging prompts\n",
        "- You need reproducible results for comparison\n",
        "- Consistency is critical (e.g., data extraction, classification)\n",
        "\n",
        "**Use higher temperature when:**\n",
        "- You want creative, diverse outputs\n",
        "- Generating multiple variations (e.g., marketing copy, brainstorming)\n",
        "- The task benefits from variety"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Automatic Prompt Generation\n",
        "\n",
        "One powerful technique is to use AI to **generate and refine prompts** for you. This is called meta-prompting or automatic prompt generation. Let's explore three practical examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Meta-Prompting - Improving a Basic Prompt\n",
        "\n",
        "Let's start with a weak prompt and use GPT to generate an improved version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start with a weak prompt\n",
        "weak_prompt = \"Explain machine learning\"\n",
        "\n",
        "print(\"Original weak prompt:\")\n",
        "print(f\"'{weak_prompt}'\\n\")\n",
        "\n",
        "# Test the weak prompt\n",
        "weak_response = call_openai(weak_prompt, temperature=0)\n",
        "display_response(\"Response from Weak Prompt\", weak_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use meta-prompting to improve it\n",
        "meta_prompt = f\"\"\"I have this prompt: \"{weak_prompt}\"\n",
        "\n",
        "Please improve this prompt to make it more effective by:\n",
        "1. Adding specific context about the target audience\n",
        "2. Specifying the desired format and length\n",
        "3. Including relevant constraints or requirements\n",
        "4. Making the output more actionable\n",
        "\n",
        "Return ONLY the improved prompt, without explanation.\"\"\"\n",
        "\n",
        "improved_prompt = call_openai(meta_prompt, temperature=0.7)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ Improved Prompt Generated:\")\n",
        "print(\"=\"*80)\n",
        "print(improved_prompt)\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the improved prompt\n",
        "improved_response = call_openai(improved_prompt, temperature=0)\n",
        "display_response(\"Response from Improved Prompt\", improved_response)\n",
        "\n",
        "print(\"\\nüìä Comparison:\")\n",
        "print(f\"Weak prompt length: {len(weak_prompt)} characters\")\n",
        "print(f\"Improved prompt length: {len(improved_prompt)} characters\")\n",
        "print(f\"Weak response length: {len(weak_response)} characters\")\n",
        "print(f\"Improved response length: {len(improved_response)} characters\")\n",
        "print(\"\\nüí° Notice how the improved prompt produces more structured, specific output!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Task-Specific Prompt Generation\n",
        "\n",
        "Let's define a specific task and have the AI generate multiple prompt variations automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a task\n",
        "task_description = \"\"\"Task: Extract key information from customer feedback emails including:\n",
        "- Customer sentiment (positive/negative/neutral)\n",
        "- Main issue or topic\n",
        "- Urgency level (low/medium/high)\n",
        "- Suggested action\"\"\"\n",
        "\n",
        "# Generate multiple prompt variations\n",
        "generation_prompt = f\"\"\"{task_description}\n",
        "\n",
        "Generate 3 different prompt variations for this task:\n",
        "1. A concise, direct prompt\n",
        "2. A detailed prompt with examples\n",
        "3. A prompt that uses role-playing\n",
        "\n",
        "Format each prompt clearly with numbers.\"\"\"\n",
        "\n",
        "prompt_variations = call_openai(generation_prompt, temperature=0.8)\n",
        "print(\"üéØ Generated Prompt Variations:\")\n",
        "print(\"=\"*80)\n",
        "print(prompt_variations)\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test one of the variations with sample data\n",
        "sample_email = \"\"\"Subject: Urgent - System keeps crashing!\n",
        "\n",
        "Hi, I'm really frustrated. Your software has crashed 3 times today and I've lost my work each time. \n",
        "This is completely unacceptable for a paid product. I need this fixed ASAP or I want a refund.\n",
        "\n",
        "- John\"\"\"\n",
        "\n",
        "# Use the first variation (adjust based on output)\n",
        "test_prompt = f\"\"\"Analyze this customer feedback and extract:\n",
        "- Sentiment\n",
        "- Main issue\n",
        "- Urgency\n",
        "- Suggested action\n",
        "\n",
        "Email: {sample_email}\"\"\"\n",
        "\n",
        "analysis = call_openai(test_prompt, temperature=0)\n",
        "display_response(\"Customer Feedback Analysis\", analysis)\n",
        "\n",
        "print(\"üí° Auto-generated prompts can help you quickly find the best approach for your task!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Iterative Refinement\n",
        "\n",
        "Create a feedback loop where the model refines its own prompt based on output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initial prompt for a task\n",
        "current_prompt = \"Write a product description for wireless earbuds\"\n",
        "iterations = 3\n",
        "\n",
        "print(\"üîÑ Iterative Prompt Refinement\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i in range(iterations):\n",
        "    print(f\"\\n### Iteration {i+1} ###\")\n",
        "    print(f\"Current prompt: {current_prompt}\")\n",
        "    \n",
        "    # Generate output with current prompt\n",
        "    output = call_openai(current_prompt, temperature=0.7)\n",
        "    print(f\"\\nOutput:\\n{output}\")\n",
        "    \n",
        "    if i < iterations - 1:  # Don't refine on last iteration\n",
        "        # Ask model to refine the prompt based on the output\n",
        "        refinement_prompt = f\"\"\"I used this prompt: \"{current_prompt}\"\n",
        "        \n",
        "And got this output: \"{output}\"\n",
        "\n",
        "The output is good but could be better. Improve the prompt to generate:\n",
        "- More specific technical details\n",
        "- Stronger emotional appeal\n",
        "- Clear call-to-action\n",
        "\n",
        "Return ONLY the improved prompt.\"\"\"\n",
        "        \n",
        "        current_prompt = call_openai(refinement_prompt, temperature=0.7)\n",
        "        print(f\"\\n‚ú® Refined prompt: {current_prompt}\")\n",
        "        print(\"-\"*80)\n",
        "        time.sleep(0.5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí° Iterative refinement helps you converge on the optimal prompt for your needs!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Token Optimization\n",
        "\n",
        "Tokens are the basic units that AI models process. **Every token costs money**, so optimizing your prompts to use fewer tokens while maintaining quality can significantly reduce costs.\n",
        "\n",
        "üí∞ As a rough approximation: 1 token ‚âà 4 characters (or 0.75 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Before/After Prompt Optimization\n",
        "\n",
        "Let's take a verbose prompt and optimize it for token efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verbose prompt (before optimization)\n",
        "verbose_prompt = \"\"\"Hello! I was wondering if you could possibly help me understand something. \n",
        "I'm trying to learn about artificial intelligence and machine learning, and I'm particularly \n",
        "interested in understanding what neural networks are and how they actually work. Could you \n",
        "please explain this to me in a way that would be easy for a beginner to understand? \n",
        "I would really appreciate it if you could provide a clear and simple explanation. Thank you!\"\"\"\n",
        "\n",
        "# Optimized prompt (after removing fluff)\n",
        "optimized_prompt = \"Explain neural networks for beginners.\"\n",
        "\n",
        "print(\"VERBOSE PROMPT:\")\n",
        "print(verbose_prompt)\n",
        "print(f\"\\nüìä Approximate tokens: {count_tokens_approx(verbose_prompt)}\")\n",
        "print(f\"Characters: {len(verbose_prompt)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nOPTIMIZED PROMPT:\")\n",
        "print(optimized_prompt)\n",
        "print(f\"\\nüìä Approximate tokens: {count_tokens_approx(optimized_prompt)}\")\n",
        "print(f\"Characters: {len(optimized_prompt)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"üí∞ Token reduction: {count_tokens_approx(verbose_prompt) - count_tokens_approx(optimized_prompt)} tokens saved!\")\n",
        "print(f\"üìâ Efficiency gain: {((count_tokens_approx(verbose_prompt) - count_tokens_approx(optimized_prompt)) / count_tokens_approx(verbose_prompt) * 100):.1f}% reduction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test both prompts to compare output quality\n",
        "print(\"Testing verbose prompt...\")\n",
        "verbose_response = call_openai(verbose_prompt, temperature=0)\n",
        "display_response(\"Verbose Prompt Response\", verbose_response)\n",
        "\n",
        "print(\"Testing optimized prompt...\")\n",
        "optimized_response = call_openai(optimized_prompt, temperature=0)\n",
        "display_response(\"Optimized Prompt Response\", optimized_response)\n",
        "\n",
        "print(\"\\nüí° Notice: The optimized prompt produces similar quality output with 85% fewer tokens!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Systematic Token Reduction Strategies\n",
        "\n",
        "Here are specific techniques to reduce token usage while maintaining clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 1: Remove filler words\n",
        "before_1 = \"Could you please help me understand how AI works?\"\n",
        "after_1 = \"Explain how AI works.\"\n",
        "\n",
        "# Strategy 2: Use abbreviations\n",
        "before_2 = \"Explain artificial intelligence and machine learning\"\n",
        "after_2 = \"Explain AI and ML\"\n",
        "\n",
        "# Strategy 3: Use imperatives instead of questions\n",
        "before_3 = \"Can you tell me what are the benefits of cloud computing?\"\n",
        "after_3 = \"List cloud computing benefits.\"\n",
        "\n",
        "# Strategy 4: Remove redundancy\n",
        "before_4 = \"Please provide a detailed and comprehensive explanation of neural networks\"\n",
        "after_4 = \"Explain neural networks comprehensively.\"\n",
        "\n",
        "strategies = [\n",
        "    (\"Remove filler words\", before_1, after_1),\n",
        "    (\"Use abbreviations\", before_2, after_2),\n",
        "    (\"Use imperatives\", before_3, after_3),\n",
        "    (\"Remove redundancy\", before_4, after_4)\n",
        "]\n",
        "\n",
        "print(\"üìã Token Optimization Strategies:\\n\")\n",
        "total_saved = 0\n",
        "\n",
        "for strategy, before, after in strategies:\n",
        "    before_tokens = count_tokens_approx(before)\n",
        "    after_tokens = count_tokens_approx(after)\n",
        "    saved = before_tokens - after_tokens\n",
        "    total_saved += saved\n",
        "    \n",
        "    print(f\"\\n{strategy}:\")\n",
        "    print(f\"  Before: \\\"{before}\\\" ({before_tokens} tokens)\")\n",
        "    print(f\"  After:  \\\"{after}\\\" ({after_tokens} tokens)\")\n",
        "    print(f\"  üí∞ Saved: {saved} tokens ({(saved/before_tokens*100):.0f}% reduction)\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"üìä Total tokens saved across examples: {total_saved}\")\n",
        "print(\"\\nüí° Apply these strategies consistently to reduce costs significantly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Calculate Cost Savings\n",
        "\n",
        "Let's calculate the actual cost difference using OpenAI's pricing (as of 2024)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenAI GPT-4o-mini pricing (example - check current pricing)\n",
        "INPUT_COST_PER_1K = 0.00015  # $0.15 per 1M tokens\n",
        "OUTPUT_COST_PER_1K = 0.0006   # $0.60 per 1M tokens\n",
        "\n",
        "def calculate_cost(input_tokens, output_tokens):\n",
        "    \"\"\"Calculate cost based on token usage\"\"\"\n",
        "    input_cost = (input_tokens / 1000) * INPUT_COST_PER_1K\n",
        "    output_cost = (output_tokens / 1000) * OUTPUT_COST_PER_1K\n",
        "    return input_cost + output_cost\n",
        "\n",
        "# Example scenario: Running 1000 queries per day\n",
        "queries_per_day = 1000\n",
        "days_per_month = 30\n",
        "\n",
        "# Scenario A: Verbose prompts\n",
        "verbose_input_tokens = 150\n",
        "verbose_output_tokens = 200\n",
        "\n",
        "# Scenario B: Optimized prompts (50% reduction)\n",
        "optimized_input_tokens = 75\n",
        "optimized_output_tokens = 200  # Output stays same\n",
        "\n",
        "# Calculate costs\n",
        "verbose_cost_per_query = calculate_cost(verbose_input_tokens, verbose_output_tokens)\n",
        "optimized_cost_per_query = calculate_cost(optimized_input_tokens, optimized_output_tokens)\n",
        "\n",
        "verbose_monthly_cost = verbose_cost_per_query * queries_per_day * days_per_month\n",
        "optimized_monthly_cost = optimized_cost_per_query * queries_per_day * days_per_month\n",
        "\n",
        "savings_per_month = verbose_monthly_cost - optimized_monthly_cost\n",
        "savings_per_year = savings_per_month * 12\n",
        "\n",
        "print(\"üí∞ COST ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nScenario: {queries_per_day} queries/day, {days_per_month} days/month\")\n",
        "print(f\"\\nVerbose Prompts:\")\n",
        "print(f\"  - Cost per query: ${verbose_cost_per_query:.6f}\")\n",
        "print(f\"  - Monthly cost: ${verbose_monthly_cost:.2f}\")\n",
        "print(f\"\\nOptimized Prompts:\")\n",
        "print(f\"  - Cost per query: ${optimized_cost_per_query:.6f}\")\n",
        "print(f\"  - Monthly cost: ${optimized_monthly_cost:.2f}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"üíµ SAVINGS:\")\n",
        "print(f\"  - Per month: ${savings_per_month:.2f}\")\n",
        "print(f\"  - Per year: ${savings_per_year:.2f}\")\n",
        "print(f\"  - Percentage saved: {(savings_per_month/verbose_monthly_cost*100):.1f}%\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"üí° Token optimization isn't just about efficiency‚Äîit directly impacts your budget!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. LLM as Judge (Brief Example)\n",
        "\n",
        "LLMs can evaluate the outputs from other LLMs. This technique is called \"LLM as Judge\" and is useful for comparing different prompts or responses.\n",
        "\n",
        "**Note:** This will be covered extensively in weeks 3, 4, and 6. Here's a quick preview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate multiple responses to evaluate\n",
        "prompt = \"Explain quantum computing in simple terms.\"\n",
        "\n",
        "responses_to_judge = []\n",
        "for i in range(3):\n",
        "    response = call_openai(prompt, temperature=0.8)\n",
        "    responses_to_judge.append(response)\n",
        "    print(f\"\\nResponse {i+1}:\")\n",
        "    print(response)\n",
        "    print(\"-\"*80)\n",
        "    time.sleep(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use LLM as judge to evaluate and rank them\n",
        "judge_prompt = f\"\"\"Evaluate these 3 explanations of quantum computing. Rate each on:\n",
        "1. Clarity (1-10)\n",
        "2. Accuracy (1-10)\n",
        "3. Accessibility for beginners (1-10)\n",
        "\n",
        "Response 1: {responses_to_judge[0]}\n",
        "\n",
        "Response 2: {responses_to_judge[1]}\n",
        "\n",
        "Response 3: {responses_to_judge[2]}\n",
        "\n",
        "Provide scores for each response and recommend the best one. Format your response clearly.\"\"\"\n",
        "\n",
        "judgment = call_openai(judge_prompt, temperature=0)\n",
        "display_response(\"LLM Judge Evaluation\", judgment)\n",
        "\n",
        "print(\"\\nüí° Note: LLM as Judge will be covered in detail in weeks 3, 4, and 6!\")\n",
        "print(\"You'll learn advanced evaluation techniques and best practices.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Biases Demonstration\n",
        "\n",
        "AI models exhibit various cognitive biases that affect their outputs. Understanding these biases helps you design better prompts to mitigate them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recency Bias\n",
        "\n",
        "Models tend to remember what's at the **end** of the prompt and forget what's at the **beginning**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test recency bias: important info at beginning vs end\n",
        "prompt_important_first = \"\"\"IMPORTANT: Your response must be exactly 2 sentences.\n",
        "\n",
        "Explain the greenhouse effect and its impact on climate change.\"\"\"\n",
        "\n",
        "prompt_important_last = \"\"\"Explain the greenhouse effect and its impact on climate change.\n",
        "\n",
        "IMPORTANT: Your response must be exactly 2 sentences.\"\"\"\n",
        "\n",
        "print(\"Testing Recency Bias...\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "response_first = call_openai(prompt_important_first, temperature=0.7)\n",
        "response_last = call_openai(prompt_important_last, temperature=0.7)\n",
        "\n",
        "print(\"Important constraint at BEGINNING:\")\n",
        "print(response_first)\n",
        "sentences_first = response_first.count('.') \n",
        "print(f\"Sentences: {sentences_first}\\n\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(\"\\nImportant constraint at END:\")\n",
        "print(response_last)\n",
        "sentences_last = response_last.count('.')\n",
        "print(f\"Sentences: {sentences_last}\\n\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üí° The model often follows the constraint better when it's at the END!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verbosity Bias\n",
        "\n",
        "Models tend to generate **long, elaborate responses** rather than brief, concise answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test verbosity bias\n",
        "prompt_brief = \"What is machine learning?\"\n",
        "prompt_explicit_brief = \"What is machine learning? Answer in one sentence.\"\n",
        "\n",
        "print(\"Testing Verbosity Bias...\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "response_natural = call_openai(prompt_brief, temperature=0.7)\n",
        "response_constrained = call_openai(prompt_explicit_brief, temperature=0.7)\n",
        "\n",
        "print(\"Without explicit brevity instruction:\")\n",
        "print(response_natural)\n",
        "print(f\"\\nLength: {len(response_natural)} characters\\n\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(\"\\nWith explicit brevity instruction:\")\n",
        "print(response_constrained)\n",
        "print(f\"\\nLength: {len(response_constrained)} characters\\n\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üí° Without constraints, models tend toward verbose responses!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List-Making Bias\n",
        "\n",
        "Models tend to format responses as **bullet points or numbered lists** rather than natural prose paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test list-making bias\n",
        "prompt_neutral = \"What are the benefits of exercise?\"\n",
        "prompt_prose = \"Write a paragraph about the benefits of exercise. Use flowing prose, not lists.\"\n",
        "\n",
        "print(\"Testing List-Making Bias...\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "response_neutral = call_openai(prompt_neutral, temperature=0.7)\n",
        "response_prose = call_openai(prompt_prose, temperature=0.7)\n",
        "\n",
        "print(\"Without format specification:\")\n",
        "print(response_neutral)\n",
        "print(f\"\\nContains bullet points or numbers: {('‚Ä¢' in response_neutral or '1.' in response_neutral or '-' in response_neutral)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"\\nWith prose instruction:\")\n",
        "print(response_prose)\n",
        "print(f\"\\nContains bullet points or numbers: {('‚Ä¢' in response_prose or '1.' in response_prose or '-' in response_prose[:50])}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí° Models default to lists‚Äîexplicitly request prose format if needed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary & Best Practices\n",
        "\n",
        "Congratulations! You've explored advanced prompt engineering techniques. Let's recap the key takeaways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaways\n",
        "\n",
        "**1. Core Techniques:**\n",
        "- Zero-shot: Clear descriptions without examples (most common)\n",
        "- One/Few-shot: Provide examples to guide format and style\n",
        "- Role prompting: Define assistant in third person for better results\n",
        "- Emotional prompting: Use emotion to influence tone\n",
        "- Chain-of-thought: Request step-by-step reasoning for complex tasks\n",
        "\n",
        "**2. Non-Deterministic Nature:**\n",
        "- LLM outputs vary between runs with same parameters\n",
        "- Use temperature=0 for consistency\n",
        "- Use seed parameter for reproducibility\n",
        "- Test prompts multiple times before deployment\n",
        "\n",
        "**3. Automatic Prompt Generation:**\n",
        "- Use AI to improve your prompts (meta-prompting)\n",
        "- Generate multiple variations automatically\n",
        "- Implement iterative refinement loops\n",
        "- Save time and discover better approaches\n",
        "\n",
        "**4. Token Optimization:**\n",
        "- Remove filler words and redundancy\n",
        "- Use abbreviations and imperatives\n",
        "- Token reduction = cost reduction\n",
        "- Maintain quality while minimizing tokens\n",
        "\n",
        "**5. Model Biases:**\n",
        "- Recency bias: Put important info at the end\n",
        "- Verbosity bias: Explicitly request brevity\n",
        "- List-making bias: Request prose format when needed\n",
        "- Design prompts to mitigate known biases\n",
        "\n",
        "**6. LLM as Judge:**\n",
        "- Use LLMs to evaluate other LLM outputs\n",
        "- Compare prompt variations objectively\n",
        "- More advanced techniques coming in weeks 3, 4, and 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Prompt Engineering Workflow\n",
        "\n",
        "Remember: Prompt engineering is an **empirical science**.\n",
        "\n",
        "```\n",
        "1. CREATE ‚Üí 2. TEST ‚Üí 3. ITERATE\n",
        "     ‚Üë                       ‚Üì\n",
        "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**Create:** Write your initial prompt based on best practices\n",
        "\n",
        "**Test:** Run it multiple times with your actual use case\n",
        "\n",
        "**Iterate:** Refine based on results, biases, and failures\n",
        "\n",
        "**Repeat:** Continue until you achieve consistent, quality outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building Your Prompt Toolkit\n",
        "\n",
        "**Save successful prompts** as templates or presets:\n",
        "- Data extraction prompts\n",
        "- Classification prompts\n",
        "- Creative writing prompts\n",
        "- Analysis and summarization prompts\n",
        "\n",
        "**Create templates** for common tasks:\n",
        "- Include placeholders for variable content\n",
        "- Document which parameters work best\n",
        "- Note any special considerations or biases\n",
        "\n",
        "**Version control** your prompts:\n",
        "- Track iterations and improvements\n",
        "- Document what works and what doesn't\n",
        "- Share learnings with your team"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick Reference: Common Problems & Solutions\n",
        "\n",
        "| Problem | Solution |\n",
        "|---------|----------|\n",
        "| **Hallucination** | Lower temperature, add examples |\n",
        "| **Off-topic** | Stronger system message, lower top-p |\n",
        "| **Too verbose** | Set max_tokens limit, use stop sequences |\n",
        "| **Repetitive** | Increase repetition penalties |\n",
        "| **Inconsistent** | Set temperature=0, use seed value |\n",
        "| **Wrong format** | Enable JSON mode, provide clear format instructions |\n",
        "| **Ignoring instructions** | Move key instructions to end (recency bias) |\n",
        "| **Too expensive** | Optimize tokens, use cheaper models where appropriate |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next Steps\n",
        "\n",
        "Now that you understand advanced prompt engineering:\n",
        "\n",
        "1. **Practice** with the techniques in this notebook\n",
        "2. **Apply** these strategies to your own projects\n",
        "3. **Experiment** with different combinations\n",
        "4. **Build** your own prompt library\n",
        "5. **Look forward** to RAG and ReAct prompting in Week 3!\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "- OpenAI Prompt Engineering Guide: https://platform.openai.com/docs/guides/prompt-engineering\n",
        "- Prompt Engineering Papers: https://github.com/dair-ai/Prompt-Engineering-Guide\n",
        "- Community Prompts: https://prompts.chat\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Prompting! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "consulting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}